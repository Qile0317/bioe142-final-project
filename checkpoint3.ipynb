{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e077c25",
   "metadata": {},
   "source": [
    "# BioE C142 Final Ugrad Project Checkpoint 3 - Qile Yang\n",
    "\n",
    "The instructions for this checkpoint are as follows: Regularization strategies and hyperparameter tuning. Use more data to train the network. Play with the architecture, hyperparameters and the regularization strategies. Show your work and defend the final choice of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6305e0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# notebook and python env setup\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torchani\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c5978c",
   "metadata": {},
   "source": [
    "## Loading previous data and functions from checkpoints 1 and 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7acf5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "691918\n"
     ]
    }
   ],
   "source": [
    "# loading data and default energy computers\n",
    "\n",
    "def init_aev_computer():\n",
    "    Rcr = 5.2\n",
    "    Rca = 3.5\n",
    "    EtaR = torch.tensor([16], dtype=torch.float, device=device)\n",
    "    ShfR = torch.tensor([\n",
    "        0.900000, 1.168750, 1.437500, 1.706250, \n",
    "        1.975000, 2.243750, 2.512500, 2.781250, \n",
    "        3.050000, 3.318750, 3.587500, 3.856250, \n",
    "        4.125000, 4.393750, 4.662500, 4.931250\n",
    "    ], dtype=torch.float, device=device)\n",
    "\n",
    "\n",
    "    EtaA = torch.tensor([8], dtype=torch.float, device=device)\n",
    "    Zeta = torch.tensor([32], dtype=torch.float, device=device)\n",
    "    ShfA = torch.tensor([0.90, 1.55, 2.20, 2.85], dtype=torch.float, device=device)\n",
    "    ShfZ = torch.tensor([\n",
    "        0.19634954, 0.58904862, 0.9817477, 1.37444680, \n",
    "        1.76714590, 2.15984490, 2.5525440, 2.94524300\n",
    "    ], dtype=torch.float, device=device)\n",
    "\n",
    "    num_species = 4\n",
    "    aev_computer = torchani.AEVComputer(\n",
    "        Rcr, Rca, EtaR, ShfR, EtaA, Zeta, ShfA, ShfZ, num_species\n",
    "    )\n",
    "    return aev_computer\n",
    "\n",
    "aev_computer = init_aev_computer()\n",
    "aev_dim = aev_computer.aev_length\n",
    "\n",
    "def load_ani_dataset(dspath):\n",
    "    self_energies = torch.tensor([ # this was in the original code, but the variable isn't used?\n",
    "        0.500607632585, -37.8302333826,\n",
    "        -54.5680045287, -75.0362229210\n",
    "    ], dtype=torch.float, device=device)\n",
    "    energy_shifter = torchani.utils.EnergyShifter(None)\n",
    "    species_order = ['H', 'C', 'N', 'O']\n",
    "\n",
    "    dataset = torchani.data.load(dspath)\n",
    "    dataset = dataset.subtract_self_energies(energy_shifter, species_order)\n",
    "    dataset = dataset.species_to_indices(species_order)\n",
    "    dataset = dataset.shuffle()\n",
    "    return dataset\n",
    "\n",
    "dataset = load_ani_dataset(\"./data/ani_gdb_s01_to_s04.h5\")\n",
    "train_data, val_data, test_data = dataset.split(0.8, 0.1, 0.1)\n",
    "print(len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07fa2651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the trainer\n",
    "\n",
    "class ANITrainer:\n",
    "    def __init__(self, model, batch_size, learning_rate, epoch, l2):\n",
    "        self.model = model\n",
    "        \n",
    "        num_params = sum(item.numel() for item in model.parameters())\n",
    "        print(f\"{model.__class__.__name__} - Number of parameters: {num_params}\")\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.optimizer = torch.optim.Adam(\n",
    "            model.parameters(), lr=learning_rate, weight_decay=l2\n",
    "        )\n",
    "        self.epoch = epoch\n",
    "    \n",
    "    def train(self, train_data, val_data, early_stop=True, draw_curve=True):\n",
    "        self.model.train()\n",
    "        \n",
    "        # init data loader\n",
    "        print(\"Initialize training data...\")\n",
    "        train_data_loader = train_data.collate(self.batch_size).cache()\n",
    "        train_data_len = len(train_data)\n",
    "        \n",
    "        # definition of loss function: MSE is a good choice! \n",
    "        loss_func = nn.MSELoss()\n",
    "        \n",
    "        # record epoch losses\n",
    "        train_loss_list = []\n",
    "        val_loss_list = []\n",
    "        lowest_val_loss = np.inf\n",
    "        \n",
    "        for i in tqdm(range(self.epoch), leave=True):\n",
    "            train_epoch_loss = 0.0\n",
    "            for train_data_batch in train_data_loader:\n",
    "                \n",
    "                # compute energies\n",
    "                true_energies = train_data_batch[\"energies\"].to(device).float()\n",
    "                \n",
    "                # compute loss\n",
    "                batch_loss = loss_func(\n",
    "                    true_energies,\n",
    "                    self.model((\n",
    "                        train_data_batch['species'].to(device),\n",
    "                        train_data_batch['coordinates'].to(device)\n",
    "                    ))[1]\n",
    "                )\n",
    "                \n",
    "                # do a step\n",
    "                self.optimizer.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                self.optimizer.step()\n",
    "                \n",
    "                batch_importance = len(train_data_batch) / train_data_len\n",
    "                train_epoch_loss += batch_loss.item() * batch_importance\n",
    "            \n",
    "            # use the self.evaluate to get loss on the validation set \n",
    "            val_epoch_loss = self.evaluate(val_data, draw_plot=False)\n",
    "            \n",
    "            # append the losses\n",
    "            train_loss_list.append(train_epoch_loss)\n",
    "            val_loss_list.append(val_epoch_loss)\n",
    "            \n",
    "            if early_stop:\n",
    "                if val_epoch_loss < lowest_val_loss:\n",
    "                    lowest_val_loss = val_epoch_loss\n",
    "                    weights = self.model.state_dict()\n",
    "        \n",
    "        if draw_curve:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4), constrained_layout=True)\n",
    "            ax.set_yscale(\"log\")\n",
    "            # Plot train loss and validation loss\n",
    "            epochs = np.arange(self.epoch)\n",
    "            ax.plot(epochs, train_loss_list, label='Train')\n",
    "            ax.plot(epochs, val_loss_list, label='Validation')\n",
    "            ax.legend()\n",
    "            ax.set_xlabel(\"# Batch\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "        \n",
    "        if early_stop:\n",
    "            self.model.load_state_dict(weights)\n",
    "        \n",
    "        return train_loss_list, val_loss_list\n",
    "    \n",
    "    \n",
    "    def evaluate(self, data, draw_plot=False):\n",
    "        \n",
    "        # init data loader\n",
    "        data_loader = data.collate(self.batch_size).cache()\n",
    "        data_len = len(data)\n",
    "        \n",
    "        # init loss function\n",
    "        loss_func = nn.MSELoss()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        if draw_plot:\n",
    "            true_energies_all = []\n",
    "            pred_energies_all = []\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            for batch_data in data_loader:\n",
    "                \n",
    "                # compute energies\n",
    "                true_energies = batch_data[\"energies\"].to(device).float()\n",
    "                _, pred_energies = self.model((\n",
    "                    batch_data['species'].to(device),\n",
    "                    batch_data['coordinates'].to(device)\n",
    "                ))\n",
    "\n",
    "                # compute loss\n",
    "                batch_loss = loss_func(true_energies, pred_energies)\n",
    "\n",
    "                batch_importance = len(batch_data) / data_len\n",
    "                total_loss += batch_loss.item() * batch_importance\n",
    "                \n",
    "                if draw_plot:\n",
    "                    true_energies_all.append(true_energies.detach().cpu().numpy().flatten())\n",
    "                    pred_energies_all.append(pred_energies.detach().cpu().numpy().flatten())\n",
    "\n",
    "        if draw_plot:\n",
    "            true_energies_all = np.concatenate(true_energies_all)\n",
    "            pred_energies_all = np.concatenate(pred_energies_all)\n",
    "            # Report the mean absolute error\n",
    "            # The unit of energies in the dataset is hartree\n",
    "            # please convert it to kcal/mol when reporting the mean absolute error\n",
    "            # 1 hartree = 627.5094738898777 kcal/mol\n",
    "            # MAE = mean(|true - pred|)\n",
    "            hartree2kcalmol = (true_energies_all - pred_energies_all) * 627.5094738898777\n",
    "            mae = np.mean(np.abs(hartree2kcalmol))\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4), constrained_layout=True)\n",
    "            ax.scatter(true_energies_all, pred_energies_all, label=f\"MAE: {mae:.2f} kcal/mol\", s=2)\n",
    "            ax.set_xlabel(\"Ground Truth\")\n",
    "            ax.set_ylabel(\"Predicted\")\n",
    "            xmin, xmax = ax.get_xlim()\n",
    "            ymin, ymax = ax.get_ylim()\n",
    "            vmin, vmax = min(xmin, ymin), max(xmax, ymax)\n",
    "            ax.set_xlim(vmin, vmax)\n",
    "            ax.set_ylim(vmin, vmax)\n",
    "            ax.plot([vmin, vmax], [vmin, vmax], color='red')\n",
    "            ax.legend()\n",
    "\n",
    "        return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e9f030",
   "metadata": {},
   "source": [
    "## Actual Checkpoint 3 Content\n",
    "\n",
    "The overarching goal is to create a better model through either changing the arcitecture or hyperparameters. (including regularization as a hyperparam) I will do this by defining several models, training on the same data, and doing grid searches on the hyperparameters. I will then compare the results and select the best one and justify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac4d9599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here I'll define a helper to create the model\n",
    "from collections.abc import Callable\n",
    "\n",
    "def create_model(model_class: Callable) -> nn.Module:\n",
    "    net_H = model_class()\n",
    "    net_C = model_class()\n",
    "    net_N = model_class()\n",
    "    net_O = model_class()\n",
    "    ani_net = torchani.ANIModel([net_H, net_C, net_N, net_O])\n",
    "    return nn.Sequential(\n",
    "        aev_computer,\n",
    "        ani_net\n",
    "    ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca60f7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential - Number of parameters: 197636\n",
      "Initialize training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [03:34<27:57, 18.85s/it]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f9391229760>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/qile/miniconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      " 12%|█▏        | 12/100 [03:54<28:19, 19.31s/it]"
     ]
    }
   ],
   "source": [
    "# I will start off by using the \"naive\" model from last time as a baseline and try to iteratively improve.\n",
    "class SingleHiddenLayerFc(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(384, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "trainer = ANITrainer(\n",
    "    model=create_model(SingleHiddenLayerFc),\n",
    "    batch_size=1024,\n",
    "    learning_rate=1e-4,\n",
    "    epoch=100,\n",
    "    l2=1e-4\n",
    ")\n",
    "\n",
    "trainer.train(train_data=train_data, val_data=val_data, early_stop=True, draw_curve=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
